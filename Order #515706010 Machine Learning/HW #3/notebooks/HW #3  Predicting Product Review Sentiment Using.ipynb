{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: Predicting Product Review Sentiment Using Classification\n",
    "\n",
    "## \tOverview\n",
    "The goal of this assignment is to build a classification machine learning (ML) pipeline in a web application to use as a tool to analyze the models to gain useful insights about model performance. Using trained classification models, build a ML application that predicts whether a product review is positive or negative.\n",
    "\n",
    "The learning outcomes for this assignment are:\n",
    "\n",
    "●\tBuild end-to-end classification pipeline with four classifiers 1) Logistic Regression, 2)Stochastic Gradient Descent, and 3) Stochastic Gradient Descent with Cross Validation.\n",
    "\n",
    "●\tEvaluate classification methods using standard metrics including precision, recall, and accuracy, ROC Curves, and area under the curve.\n",
    "\n",
    "●\tDevelop a web application that walks users through steps of the classification pipeline and provide tools to analyze multiple methods across multiple metrics.\n",
    "\n",
    "●\tDevelop a web application that classifies products as positive or negative and indicates the cost of displaying false positives and false negatives using a specified model.\n",
    "\n",
    "## 3.1\tAmazon Product Reviews Dataset\n",
    "\n",
    "This assignment involves training and evaluating ML end-to-end pipeline in a web application using the Amazon Product Reviews dataset. Millions of Amazon customers have contributed over a hundred million reviews to express opinions and describe their experiences regarding products on the Amazon.com website. This makes Amazon Customer Reviews a rich source of information for academic researchers in the fields of natural language processing (NLP), information retrieval (IR), and machine learning (ML), amongst others. Specifically, this dataset was constructed to represent a sample of customer evaluations and opinions, variation in the perception of a product across geographical regions, and promotional intent or bias in reviews.\n",
    "We have added additional features to the dataset. There are many features, but the important ones include:\n",
    "\n",
    "●\tname: name of Amazon product\n",
    "\n",
    "●\tReviews.text: text in review\n",
    "\n",
    "●\tReviews.title: title of reviews\n",
    "\n",
    "### 3.2\tExplore and Preprocess Data (3 points)\n",
    "The goal of this page is to explore and preprocess the dataset. First, import the dataset from your machine. We have provided code to remove unuseful features using the clean_data() helper function (see helper_function.py). Then, remove punctuation from the reviews. We have provided UI and functions to summarize text statistics from reviews, search reviews with a keyword, and remove reviews. At the end of this page, encode documents with word counts and Term Frequency Inverse Document frequency features. See details about the checkpoint functions below.\n",
    "Some activities require a try and except block to train classification models (in later sections).\n",
    "``` try:\n",
    "# write some code Except ValueError as err:\n",
    "st.write({str(err)}) # Print the error message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries and helper functions\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import helper_functions\n",
    "import string\n",
    "import streamlit as st\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedKFold\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>asins</th>\n",
       "      <th>brand</th>\n",
       "      <th>categories</th>\n",
       "      <th>colors</th>\n",
       "      <th>dateAdded</th>\n",
       "      <th>dateUpdated</th>\n",
       "      <th>dimension</th>\n",
       "      <th>ean</th>\n",
       "      <th>keys</th>\n",
       "      <th>...</th>\n",
       "      <th>reviews.rating</th>\n",
       "      <th>reviews.sourceURLs</th>\n",
       "      <th>reviews.text</th>\n",
       "      <th>reviews.title</th>\n",
       "      <th>reviews.userCity</th>\n",
       "      <th>reviews.userProvince</th>\n",
       "      <th>reviews.username</th>\n",
       "      <th>sizes</th>\n",
       "      <th>upc</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
       "      <td>B00QJDU3KY</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Amazon Devices,mazon.co.uk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-03-08T20:21:53Z</td>\n",
       "      <td>2017-07-18T23:52:58Z</td>\n",
       "      <td>169 mm x 117 mm x 9.1 mm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kindlepaperwhite/b00qjdu3ky</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>https://www.amazon.com/Kindle-Paperwhite-High-...</td>\n",
       "      <td>I initially had trouble deciding between the p...</td>\n",
       "      <td>Paperwhite voyage, no regrets!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cristina M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>205 grams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
       "      <td>B00QJDU3KY</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Amazon Devices,mazon.co.uk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-03-08T20:21:53Z</td>\n",
       "      <td>2017-07-18T23:52:58Z</td>\n",
       "      <td>169 mm x 117 mm x 9.1 mm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kindlepaperwhite/b00qjdu3ky</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>https://www.amazon.com/Kindle-Paperwhite-High-...</td>\n",
       "      <td>Allow me to preface this with a little history...</td>\n",
       "      <td>One Simply Could Not Ask For More</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ricky</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>205 grams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
       "      <td>B00QJDU3KY</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Amazon Devices,mazon.co.uk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-03-08T20:21:53Z</td>\n",
       "      <td>2017-07-18T23:52:58Z</td>\n",
       "      <td>169 mm x 117 mm x 9.1 mm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kindlepaperwhite/b00qjdu3ky</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>https://www.amazon.com/Kindle-Paperwhite-High-...</td>\n",
       "      <td>I am enjoying it so far. Great for reading. Ha...</td>\n",
       "      <td>Great for those that just want an e-reader</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tedd Gardiner</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>205 grams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
       "      <td>B00QJDU3KY</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Amazon Devices,mazon.co.uk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-03-08T20:21:53Z</td>\n",
       "      <td>2017-07-18T23:52:58Z</td>\n",
       "      <td>169 mm x 117 mm x 9.1 mm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kindlepaperwhite/b00qjdu3ky</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>https://www.amazon.com/Kindle-Paperwhite-High-...</td>\n",
       "      <td>I bought one of the first Paperwhites and have...</td>\n",
       "      <td>Love / Hate relationship</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dougal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>205 grams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
       "      <td>B00QJDU3KY</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Amazon Devices,mazon.co.uk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-03-08T20:21:53Z</td>\n",
       "      <td>2017-07-18T23:52:58Z</td>\n",
       "      <td>169 mm x 117 mm x 9.1 mm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kindlepaperwhite/b00qjdu3ky</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>https://www.amazon.com/Kindle-Paperwhite-High-...</td>\n",
       "      <td>I have to say upfront - I don't like coroporat...</td>\n",
       "      <td>I LOVE IT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Miljan David Tanic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>205 grams</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id       asins   brand                  categories  \\\n",
       "0  AVpe7AsMilAPnD_xQ78G  B00QJDU3KY  Amazon  Amazon Devices,mazon.co.uk   \n",
       "1  AVpe7AsMilAPnD_xQ78G  B00QJDU3KY  Amazon  Amazon Devices,mazon.co.uk   \n",
       "2  AVpe7AsMilAPnD_xQ78G  B00QJDU3KY  Amazon  Amazon Devices,mazon.co.uk   \n",
       "3  AVpe7AsMilAPnD_xQ78G  B00QJDU3KY  Amazon  Amazon Devices,mazon.co.uk   \n",
       "4  AVpe7AsMilAPnD_xQ78G  B00QJDU3KY  Amazon  Amazon Devices,mazon.co.uk   \n",
       "\n",
       "  colors             dateAdded           dateUpdated  \\\n",
       "0    NaN  2016-03-08T20:21:53Z  2017-07-18T23:52:58Z   \n",
       "1    NaN  2016-03-08T20:21:53Z  2017-07-18T23:52:58Z   \n",
       "2    NaN  2016-03-08T20:21:53Z  2017-07-18T23:52:58Z   \n",
       "3    NaN  2016-03-08T20:21:53Z  2017-07-18T23:52:58Z   \n",
       "4    NaN  2016-03-08T20:21:53Z  2017-07-18T23:52:58Z   \n",
       "\n",
       "                  dimension  ean                         keys  ...  \\\n",
       "0  169 mm x 117 mm x 9.1 mm  NaN  kindlepaperwhite/b00qjdu3ky  ...   \n",
       "1  169 mm x 117 mm x 9.1 mm  NaN  kindlepaperwhite/b00qjdu3ky  ...   \n",
       "2  169 mm x 117 mm x 9.1 mm  NaN  kindlepaperwhite/b00qjdu3ky  ...   \n",
       "3  169 mm x 117 mm x 9.1 mm  NaN  kindlepaperwhite/b00qjdu3ky  ...   \n",
       "4  169 mm x 117 mm x 9.1 mm  NaN  kindlepaperwhite/b00qjdu3ky  ...   \n",
       "\n",
       "  reviews.rating                                 reviews.sourceURLs  \\\n",
       "0            5.0  https://www.amazon.com/Kindle-Paperwhite-High-...   \n",
       "1            5.0  https://www.amazon.com/Kindle-Paperwhite-High-...   \n",
       "2            4.0  https://www.amazon.com/Kindle-Paperwhite-High-...   \n",
       "3            5.0  https://www.amazon.com/Kindle-Paperwhite-High-...   \n",
       "4            5.0  https://www.amazon.com/Kindle-Paperwhite-High-...   \n",
       "\n",
       "                                        reviews.text  \\\n",
       "0  I initially had trouble deciding between the p...   \n",
       "1  Allow me to preface this with a little history...   \n",
       "2  I am enjoying it so far. Great for reading. Ha...   \n",
       "3  I bought one of the first Paperwhites and have...   \n",
       "4  I have to say upfront - I don't like coroporat...   \n",
       "\n",
       "                                reviews.title reviews.userCity  \\\n",
       "0              Paperwhite voyage, no regrets!              NaN   \n",
       "1           One Simply Could Not Ask For More              NaN   \n",
       "2  Great for those that just want an e-reader              NaN   \n",
       "3                    Love / Hate relationship              NaN   \n",
       "4                                   I LOVE IT              NaN   \n",
       "\n",
       "  reviews.userProvince    reviews.username  sizes upc     weight  \n",
       "0                  NaN          Cristina M    NaN NaN  205 grams  \n",
       "1                  NaN               Ricky    NaN NaN  205 grams  \n",
       "2                  NaN       Tedd Gardiner    NaN NaN  205 grams  \n",
       "3                  NaN              Dougal    NaN NaN  205 grams  \n",
       "4                  NaN  Miljan David Tanic    NaN NaN  205 grams  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data\n",
    "products = pd.read_csv('datasets/Amazon Product Reviews I.csv')\n",
    "products.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-23 11:11:56.131 WARNING streamlit.runtime.state.session_state_proxy: Session state does not function when running a script without `streamlit run`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I initially had trouble deciding between the p...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Paperwhite voyage, no regrets!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Allow me to preface this with a little history...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>One Simply Could Not Ask For More</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am enjoying it so far. Great for reading. Ha...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Great for those that just want an e-reader</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I bought one of the first Paperwhites and have...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Love / Hate relationship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I have to say upfront - I don't like coroporat...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I LOVE IT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  rating  \\\n",
       "0  I initially had trouble deciding between the p...     5.0   \n",
       "1  Allow me to preface this with a little history...     5.0   \n",
       "2  I am enjoying it so far. Great for reading. Ha...     4.0   \n",
       "3  I bought one of the first Paperwhites and have...     5.0   \n",
       "4  I have to say upfront - I don't like coroporat...     5.0   \n",
       "\n",
       "                                        title  \n",
       "0              Paperwhite voyage, no regrets!  \n",
       "1           One Simply Could Not Ask For More  \n",
       "2  Great for those that just want an e-reader  \n",
       "3                    Love / Hate relationship  \n",
       "4                                   I LOVE IT  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove unuseful features \n",
    "products=helper_functions.clean_data(products)[0]\n",
    "products.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint 1:** Start cleaning the text by removing punctuation from features in the dataset.\n",
    "To do this, fill in code for the remove_punctuation function, which takes the following inputs:\n",
    "the pandas dataframe (df), and a list of the feature(s) to clean. The function returns a dataframe with updated features with removed punctuation (df).\n",
    "Perform the following tasks in the remove_punctuation function:\n",
    "1.\tCreate a translator using the string library that creates a one to one mapping of a character to its translation/replacement.\n",
    "2.\tWrite a for loop that iterates through the feature names, check that strings are in the feature.\n",
    "a.\tIf the features are strings, use the translator to remove punctuation from the strings. It’s recommended that you use a lambda function.\n",
    "3.\tStore the updated dataframe df in st.session_state[‘data’].\n",
    "\n",
    "Example code: ```\n",
    "translator = str.maketrans('', '', string.punctuation) for feature_name in features:\n",
    "if(df[feature_name].dtype ==’object’):\n",
    "df[feature_name] = … # add code here ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I initially had trouble deciding between the p...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Paperwhite voyage no regrets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Allow me to preface this with a little history...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>One Simply Could Not Ask For More</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am enjoying it so far Great for reading Had ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Great for those that just want an ereader</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I bought one of the first Paperwhites and have...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Love  Hate relationship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I have to say upfront  I dont like coroporate ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I LOVE IT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  rating  \\\n",
       "0  I initially had trouble deciding between the p...     5.0   \n",
       "1  Allow me to preface this with a little history...     5.0   \n",
       "2  I am enjoying it so far Great for reading Had ...     4.0   \n",
       "3  I bought one of the first Paperwhites and have...     5.0   \n",
       "4  I have to say upfront  I dont like coroporate ...     5.0   \n",
       "\n",
       "                                       title  \n",
       "0               Paperwhite voyage no regrets  \n",
       "1          One Simply Could Not Ask For More  \n",
       "2  Great for those that just want an ereader  \n",
       "3                    Love  Hate relationship  \n",
       "4                                  I LOVE IT  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punctuation(df, features):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    for feature in features:\n",
    "        if df[feature].dtype == 'object':\n",
    "            df[feature] = df[feature].apply(lambda x: x.translate(translator))\n",
    "    st.session_state['data'] = df\n",
    "    return df\n",
    "\n",
    "products = remove_punctuation(products, ['reviews', 'title'])\n",
    "products.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint 2:** Words need to be encoded as integers or floating point values to input to a machine learning algorithm. Your task is to perform word frequency encoding in the word_count_encoder function, which takes four inputs: the pandas dataframe (df), a list of the feature(s) to perform work count encoding on the given features (feature) and a list of strings with word encoding names 'TF-IDF', 'Word Count' (word_encoder). The function performs work count encoding on the given features and returns the data frame with word count encoded features (df).\n",
    "Perform the following tasks in the word_count_encoder function:\n",
    "1.\tUse the CountVectorizer() to create a count vectorizer class object.\n",
    "2.\tUse the count vectorizer transform() function to the feature in df to create frequency counts for words.\n",
    "3.\tConvert the frequency counts to an array using the toarray() function and convert the array to a pandas dataframe.\n",
    "4.\tAdd a prefix to the column names in the data frame created in Step 3 using add_prefix() pandas function with ‘word_count_’ as the prefix.\n",
    "5.\tAdd the word count dataframe to df using the pd.concat() function.\n",
    "6.\tUpdate the confirmation statement to show the length of the word_count dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count encoding has been performed. Added 6750 columns to the dataframe.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "      <th>word_count_0</th>\n",
       "      <th>word_count_1</th>\n",
       "      <th>word_count_2</th>\n",
       "      <th>word_count_3</th>\n",
       "      <th>word_count_4</th>\n",
       "      <th>word_count_5</th>\n",
       "      <th>word_count_6</th>\n",
       "      <th>...</th>\n",
       "      <th>word_count_6740</th>\n",
       "      <th>word_count_6741</th>\n",
       "      <th>word_count_6742</th>\n",
       "      <th>word_count_6743</th>\n",
       "      <th>word_count_6744</th>\n",
       "      <th>word_count_6745</th>\n",
       "      <th>word_count_6746</th>\n",
       "      <th>word_count_6747</th>\n",
       "      <th>word_count_6748</th>\n",
       "      <th>word_count_6749</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I initially had trouble deciding between the p...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Paperwhite voyage no regrets</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Allow me to preface this with a little history...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>One Simply Could Not Ask For More</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am enjoying it so far Great for reading Had ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Great for those that just want an ereader</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I bought one of the first Paperwhites and have...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Love  Hate relationship</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I have to say upfront  I dont like coroporate ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I LOVE IT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6753 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  rating  \\\n",
       "0  I initially had trouble deciding between the p...     5.0   \n",
       "1  Allow me to preface this with a little history...     5.0   \n",
       "2  I am enjoying it so far Great for reading Had ...     4.0   \n",
       "3  I bought one of the first Paperwhites and have...     5.0   \n",
       "4  I have to say upfront  I dont like coroporate ...     5.0   \n",
       "\n",
       "                                       title  word_count_0  word_count_1  \\\n",
       "0               Paperwhite voyage no regrets             0             0   \n",
       "1          One Simply Could Not Ask For More             0             0   \n",
       "2  Great for those that just want an ereader             0             0   \n",
       "3                    Love  Hate relationship             0             0   \n",
       "4                                  I LOVE IT             0             0   \n",
       "\n",
       "   word_count_2  word_count_3  word_count_4  word_count_5  word_count_6  ...  \\\n",
       "0             0             0             0             0             0  ...   \n",
       "1             0             0             0             0             0  ...   \n",
       "2             0             0             0             0             0  ...   \n",
       "3             0             0             0             0             0  ...   \n",
       "4             0             0             0             0             0  ...   \n",
       "\n",
       "   word_count_6740  word_count_6741  word_count_6742  word_count_6743  \\\n",
       "0                0                0                0                0   \n",
       "1                0                0                0                0   \n",
       "2                0                0                0                0   \n",
       "3                0                0                0                0   \n",
       "4                0                0                0                0   \n",
       "\n",
       "   word_count_6744  word_count_6745  word_count_6746  word_count_6747  \\\n",
       "0                0                0                0                0   \n",
       "1                0                0                0                0   \n",
       "2                0                0                0                0   \n",
       "3                0                0                0                0   \n",
       "4                0                0                0                0   \n",
       "\n",
       "   word_count_6748  word_count_6749  \n",
       "0                0                0  \n",
       "1                0                0  \n",
       "2                0                0  \n",
       "3                0                0  \n",
       "4                0                0  \n",
       "\n",
       "[5 rows x 6753 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_count_encoder(df, features, word_encoder):\n",
    "    for feature in features:\n",
    "        # create a CountVectorizer object\n",
    "        count_vect = CountVectorizer()\n",
    "\n",
    "        # fit and transform the feature using the CountVectorizer\n",
    "        freq_counts = count_vect.fit_transform(df[feature])\n",
    "\n",
    "        # convert the resulting sparse matrix to a dense array and then to a dataframe\n",
    "        freq_counts_df = pd.DataFrame(freq_counts.toarray())\n",
    "\n",
    "        # add prefix to column names\n",
    "        freq_counts_df = freq_counts_df.add_prefix('word_count_')\n",
    "\n",
    "        # concatenate the resulting word count dataframe to the original dataframe\n",
    "        df = pd.concat([df, freq_counts_df], axis=1)\n",
    "\n",
    "    # print confirmation message\n",
    "    if 'Word Count' in word_encoder:\n",
    "        print(f\"Word count encoding has been performed. Added {len(freq_counts_df.columns)} columns to the dataframe.\")\n",
    "    return df\n",
    "\n",
    "products= word_count_encoder(products, ['reviews'], ['Word Count'])\n",
    "products.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint 3:** Next, we want to perform TF-IDF encoding, which quantifies the importance or \n",
    "relevance of words or phrases. Fill in code for the tf_idf_encoder function, which takes three \n",
    "inputs: the pandas dataframe (df), a list of the feature(s) to perform tf-idf encoding on\n",
    "(feature), and a list of strings with word encoding names ‘TF-IDF’, ‘Word Count’ (word_encoder). \n",
    "This function returns the dataframe with TF-IDF encoded feature(s).\n",
    "Perform the following tasks in the tf_idf_word_count_encoder function:\n",
    "1. Use the CountVectorizer() to create a count vectorizer class object.\n",
    "2. Use the count vectorizer transform() function to the feature in df to create frequency \n",
    "counts for words.\n",
    "3. Use the TfidfTransformer() to create a TF-IDF transformer class object.\n",
    "4. Transform the frequency counts (from Step 2) into TF-IDF features using the \n",
    "TfidfTransformer object.\n",
    "5. Create a pandas dataframe for the TF-IDF features which takes the TF-IDF features array \n",
    "as input so convert the TF-IDF features to an array using the toarray() function.\n",
    "6. Add a prefix to the column names in the data frame created in Step 3 using add_prefix() \n",
    "pandas function with ‘tf_idf_word_count_’ as the prefix.\n",
    "7. Add the TF-IDF dataframe to df using the pd.concat() function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "      <th>word_count_0</th>\n",
       "      <th>word_count_1</th>\n",
       "      <th>word_count_2</th>\n",
       "      <th>word_count_3</th>\n",
       "      <th>word_count_4</th>\n",
       "      <th>word_count_5</th>\n",
       "      <th>word_count_6</th>\n",
       "      <th>...</th>\n",
       "      <th>tf_idf_word_count_6740</th>\n",
       "      <th>tf_idf_word_count_6741</th>\n",
       "      <th>tf_idf_word_count_6742</th>\n",
       "      <th>tf_idf_word_count_6743</th>\n",
       "      <th>tf_idf_word_count_6744</th>\n",
       "      <th>tf_idf_word_count_6745</th>\n",
       "      <th>tf_idf_word_count_6746</th>\n",
       "      <th>tf_idf_word_count_6747</th>\n",
       "      <th>tf_idf_word_count_6748</th>\n",
       "      <th>tf_idf_word_count_6749</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I initially had trouble deciding between the p...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Paperwhite voyage no regrets</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Allow me to preface this with a little history...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>One Simply Could Not Ask For More</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am enjoying it so far Great for reading Had ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Great for those that just want an ereader</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I bought one of the first Paperwhites and have...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Love  Hate relationship</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I have to say upfront  I dont like coroporate ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I LOVE IT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 13503 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  rating  \\\n",
       "0  I initially had trouble deciding between the p...     5.0   \n",
       "1  Allow me to preface this with a little history...     5.0   \n",
       "2  I am enjoying it so far Great for reading Had ...     4.0   \n",
       "3  I bought one of the first Paperwhites and have...     5.0   \n",
       "4  I have to say upfront  I dont like coroporate ...     5.0   \n",
       "\n",
       "                                       title  word_count_0  word_count_1  \\\n",
       "0               Paperwhite voyage no regrets             0             0   \n",
       "1          One Simply Could Not Ask For More             0             0   \n",
       "2  Great for those that just want an ereader             0             0   \n",
       "3                    Love  Hate relationship             0             0   \n",
       "4                                  I LOVE IT             0             0   \n",
       "\n",
       "   word_count_2  word_count_3  word_count_4  word_count_5  word_count_6  ...  \\\n",
       "0             0             0             0             0             0  ...   \n",
       "1             0             0             0             0             0  ...   \n",
       "2             0             0             0             0             0  ...   \n",
       "3             0             0             0             0             0  ...   \n",
       "4             0             0             0             0             0  ...   \n",
       "\n",
       "   tf_idf_word_count_6740  tf_idf_word_count_6741  tf_idf_word_count_6742  \\\n",
       "0                     0.0                     0.0                     0.0   \n",
       "1                     0.0                     0.0                     0.0   \n",
       "2                     0.0                     0.0                     0.0   \n",
       "3                     0.0                     0.0                     0.0   \n",
       "4                     0.0                     0.0                     0.0   \n",
       "\n",
       "   tf_idf_word_count_6743  tf_idf_word_count_6744  tf_idf_word_count_6745  \\\n",
       "0                     0.0                     0.0                     0.0   \n",
       "1                     0.0                     0.0                     0.0   \n",
       "2                     0.0                     0.0                     0.0   \n",
       "3                     0.0                     0.0                     0.0   \n",
       "4                     0.0                     0.0                     0.0   \n",
       "\n",
       "   tf_idf_word_count_6746  tf_idf_word_count_6747  tf_idf_word_count_6748  \\\n",
       "0                     0.0                     0.0                     0.0   \n",
       "1                     0.0                     0.0                     0.0   \n",
       "2                     0.0                     0.0                     0.0   \n",
       "3                     0.0                     0.0                     0.0   \n",
       "4                     0.0                     0.0                     0.0   \n",
       "\n",
       "   tf_idf_word_count_6749  \n",
       "0                     0.0  \n",
       "1                     0.0  \n",
       "2                     0.0  \n",
       "3                     0.0  \n",
       "4                     0.0  \n",
       "\n",
       "[5 rows x 13503 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def tf_idf_encoder(df, features, word_encoder):\n",
    "    for feature in features:\n",
    "        # create a CountVectorizer object\n",
    "        count_vect = CountVectorizer()\n",
    "\n",
    "        # fit and transform the feature using the CountVectorizer\n",
    "        freq_counts = count_vect.fit_transform(df[feature])\n",
    "\n",
    "        # create a TfidfTransformer object\n",
    "        tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "        # fit and transform the frequency counts using the TfidfTransformer\n",
    "        tfidf_features = tfidf_transformer.fit_transform(freq_counts)\n",
    "\n",
    "        # convert the resulting sparse matrix to a dense array and then to a dataframe\n",
    "        tfidf_features_df = pd.DataFrame(tfidf_features.toarray())\n",
    "\n",
    "        # add prefix to column names\n",
    "        tfidf_features_df = tfidf_features_df.add_prefix('tf_idf_word_count_')\n",
    "\n",
    "        # concatenate the resulting tf-idf dataframe to the original dataframe\n",
    "        df = pd.concat([df, tfidf_features_df], axis=1)\n",
    "\n",
    "    # print confirmation message\n",
    "    if 'TF-IDF' in word_encoder:\n",
    "        print(f\"TF-IDF encoding has been performed. Added {len(tfidf_features_df.columns)} columns to the dataframe.\")\n",
    "    return df\n",
    "products = tf_idf_encoder(products, ['reviews'], ['Word Count'])\n",
    "products.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Train Regression Models (6 points)\n",
    "The goal of this page is to train multiple models and inspect model coefficients and cross validation \n",
    "results for relevant models. First, we have provided code to assign the negative values to the \n",
    "product ratings using the negative ratings selected from the user (assume rating=3 is neural) in \n",
    "the set_pos_neg_reviews() function. Then, write the split_dataset() function which splits the \n",
    "dataset into training and validation input and output using the appropriate word encoding (as \n",
    "specified by the user). Next, write four functions to training functions to train the following \n",
    "models: 1) Logistic Regression, 2) Stochastic Gradient Descent, and 3) Stochastic Gradient \n",
    "Descent with Cross Validation. Lastly, write a function to inspect the coefficients of each model.\n",
    "\n",
    "**Checkpoint 4:** Before training the models, you need to split the data set into training and test \n",
    "sets. Complete the split_dataset function which takes the following inputs: training features (X), \n",
    "training targets (y), the ratio of test samples (number). As input, pass the data matrix X along \n",
    "with the corresponding target vector y into scikit-learn’s train_test_split() function. Set the default \n",
    "test_size to 0.2, and the default random_state to 42. The function will output four objects, in the \n",
    "following order: X_train, X_val, y_train, y_val. Refer to the scikit-learn train_test_split() function \n",
    "for help.\n",
    "```\n",
    "(df, number, test_size, target, feature_encoding, random_state=42)\n",
    "```\n",
    "Perform the following tasks in the split_dataset function:\n",
    "1. Use the train_test_split() function to split the dataset into four parts including X_train, \n",
    "X_val, y_train, y_val sets using the input X, y, number/100 (set test percentage), and \n",
    "random state.\n",
    "2. Check the feature_encoding list of strings that contain either ‘TF-IDF’ or ‘Word Count’ ing \n",
    "the feature_encoding list and set the input to feature names that start with \n",
    "‘tf_idf_word_count_’ for TF-IDF and ‘word_count_’ for word count (see example below). \n",
    "Also, the dataset can contain both feature encodings.\n",
    "``` if(‘Word Count’ in \n",
    "feature_encoding):\n",
    "X_train_sentiment = X_train.loc[:, X_train.columns.str.startswith('word_count_')]\n",
    "X_val_sentiment = X_val.loc[:, X_val.columns.str.startswith('word_count_')]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, y, number, feature_encoding, random_state=42):\n",
    "    # split the dataset into train and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=number/100, random_state=random_state)\n",
    "    \n",
    "    # check if the feature encoding is Word Count or TF-IDF and extract the corresponding features\n",
    "    if 'Word Count' in feature_encoding:\n",
    "        X_train = X_train.loc[:, X_train.columns.str.startswith('word_count_')]\n",
    "        X_val = X_val.loc[:, X_val.columns.str.startswith('word_count_')]\n",
    "    elif 'TF-IDF' in feature_encoding:\n",
    "        X_train = X_train.loc[:, X_train.columns.str.startswith('tf_idf_word_count_')]\n",
    "        X_val = X_val.loc[:, X_val.columns.str.startswith('tf_idf_word_count_')]\n",
    "        \n",
    "    return X_train, X_val, y_train, y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = products.drop(columns=['rating'])\n",
    "y = products['rating']\n",
    "X_train, X_val, y_train, y_val =split_dataset(X, y, number=20, feature_encoding=['Word Count'], random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint 5:** Now it is time to train the model with logistic regression and store it in \n",
    "st.session_state[model_name]. Complete the train_logistic_regression function which takes the \n",
    "following inputs: training features, training targets, a string of the model name, and a dictionary \n",
    "with the logistic regression hyperparameters max_iter, solver, tol, and penalty (X_train, y_train, \n",
    "model_name, params, random_state=42). The function outputs the trained model (lg_model).\n",
    "Perform the following tasks in the train_logistic_regression function:\n",
    "1. Create a try and except block to train a logistic regression model.\n",
    "2. Create a LogisticRegression class object using the random_state as input.\n",
    "3. Fit the model to the data using the fit() function with input data X_train, y_train.\n",
    "Remember to create a continuous y_train array using np.ravel() function.\n",
    "4. Save the model in st.session_state[model_name].\n",
    "5. Return the trained mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1053, 6753)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products = products[products['rating'] != 3]\n",
    "products.reset_index(drop=True, inplace=True)\n",
    "products.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "      <th>word_count_0</th>\n",
       "      <th>word_count_1</th>\n",
       "      <th>word_count_2</th>\n",
       "      <th>word_count_3</th>\n",
       "      <th>word_count_4</th>\n",
       "      <th>word_count_5</th>\n",
       "      <th>word_count_6</th>\n",
       "      <th>...</th>\n",
       "      <th>word_count_6741</th>\n",
       "      <th>word_count_6742</th>\n",
       "      <th>word_count_6743</th>\n",
       "      <th>word_count_6744</th>\n",
       "      <th>word_count_6745</th>\n",
       "      <th>word_count_6746</th>\n",
       "      <th>word_count_6747</th>\n",
       "      <th>word_count_6748</th>\n",
       "      <th>word_count_6749</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I initially had trouble deciding between the p...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Paperwhite voyage no regrets</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 6754 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  rating  \\\n",
       "0  I initially had trouble deciding between the p...     5.0   \n",
       "\n",
       "                          title  word_count_0  word_count_1  word_count_2  \\\n",
       "0  Paperwhite voyage no regrets             0             0             0   \n",
       "\n",
       "   word_count_3  word_count_4  word_count_5  word_count_6  ...  \\\n",
       "0             0             0             0             0  ...   \n",
       "\n",
       "   word_count_6741  word_count_6742  word_count_6743  word_count_6744  \\\n",
       "0                0                0                0                0   \n",
       "\n",
       "   word_count_6745  word_count_6746  word_count_6747  word_count_6748  \\\n",
       "0                0                0                0                0   \n",
       "\n",
       "   word_count_6749  sentiment  \n",
       "0                0          1  \n",
       "\n",
       "[1 rows x 6754 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products['sentiment'] = products['rating'].apply(lambda r : +1 if r > 3 else -1)\n",
    "products.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_dataset(df, number, target, feature_encoding, random_state=42):\n",
    "    \"\"\"\n",
    "    This function splits the dataset into the training and test sets.\n",
    "\n",
    "    Input:\n",
    "        - X: training features\n",
    "        - y: training targets\n",
    "        - number: the ratio of test samples\n",
    "        - target: article feature name 'rating'\n",
    "        - feature_encoding: (string) 'Word Count' or 'TF-IDF' encoding\n",
    "        - random_state: determines random number generation for centroid initialization\n",
    "    Output:\n",
    "        - X_train_sentiment: training features (word encoded)\n",
    "        - X_val_sentiment: test/validation features (word encoded)\n",
    "        - y_train: training targets\n",
    "        - y_val: test/validation targets\n",
    "    \"\"\"\n",
    "    X_train, X_val, y_train, y_val = [], [], [], []\n",
    "    X_train_sentiment, X_val_sentiment = [], []\n",
    "    try:\n",
    "        # Split dataset into y (target='sentiment') and X (all other features)\n",
    "        X = df.drop(columns=[target])\n",
    "        y = df[target]\n",
    "\n",
    "        # Split the train and test sets into X_train, X_val, y_train, y_val using X, y, number/100, and random_state\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=number/100, random_state=random_state)\n",
    "\n",
    "        # Use the column word_count and tf_idf_word_count as a feature prefix on X_train and X_val sets\n",
    "        if 'Word Count' in feature_encoding:\n",
    "            X_train_sentiment = X_train.loc[:, X_train.columns.str.startswith('word_count_')]\n",
    "            X_val_sentiment = X_val.loc[:, X_val.columns.str.startswith('word_count_')]\n",
    "        elif 'TF-IDF' in feature_encoding:\n",
    "            X_train_sentiment = X_train.loc[:, X_train.columns.str.startswith('tf_idf_word_count_')]\n",
    "            X_val_sentiment = X_val.loc[:, X_val.columns.str.startswith('tf_idf_word_count_')]\n",
    "\n",
    "        # Compute dataset percentages\n",
    "        train_percentage = (len(X_train) /\n",
    "                            (len(X_train)+len(X_val)))*100\n",
    "        test_percentage = (len(X_val) /\n",
    "                           (len(X_train)+len(X_val)))*100\n",
    "\n",
    "        # Print dataset split result\n",
    "        st.markdown('The training dataset contains {0:.2f} observations ({1:.2f}%) and the test dataset contains {2:.2f} observations ({3:.2f}%).'.format(len(X_train),\n",
    "                                                                                                                                                          train_percentage,\n",
    "                                                                                                                                                          len(\n",
    "                                                                                                                                                              X_val),\n",
    "                                                                                                                                                          test_percentage))\n",
    "\n",
    "        # (Uncomment code) Save train and test split to st.session_state\n",
    "        st.session_state['X_train'] = X_train_sentiment\n",
    "        st.session_state['X_val'] = X_val_sentiment\n",
    "        st.session_state['y_train'] = y_train\n",
    "        st.session_state['y_val'] = y_val\n",
    "    except:\n",
    "        print('Exception thrown; testing test size to 0')\n",
    "    return X_train_sentiment, X_val_sentiment, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = products.drop(columns=['rating'])\n",
    "y = products['rating']\n",
    "X_train, X_val, y_train, y_val =split_dataset(products,target='sentiment',number=20, feature_encoding=['Word Count'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_count_0</th>\n",
       "      <th>word_count_1</th>\n",
       "      <th>word_count_2</th>\n",
       "      <th>word_count_3</th>\n",
       "      <th>word_count_4</th>\n",
       "      <th>word_count_5</th>\n",
       "      <th>word_count_6</th>\n",
       "      <th>word_count_7</th>\n",
       "      <th>word_count_8</th>\n",
       "      <th>word_count_9</th>\n",
       "      <th>...</th>\n",
       "      <th>word_count_6740</th>\n",
       "      <th>word_count_6741</th>\n",
       "      <th>word_count_6742</th>\n",
       "      <th>word_count_6743</th>\n",
       "      <th>word_count_6744</th>\n",
       "      <th>word_count_6745</th>\n",
       "      <th>word_count_6746</th>\n",
       "      <th>word_count_6747</th>\n",
       "      <th>word_count_6748</th>\n",
       "      <th>word_count_6749</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>842 rows × 6750 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      word_count_0  word_count_1  word_count_2  word_count_3  word_count_4  \\\n",
       "786              0             0             0             0             0   \n",
       "261              0             0             0             0             0   \n",
       "299              0             0             0             0             0   \n",
       "713              0             0             0             0             0   \n",
       "990              0             0             0             0             0   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "330              0             0             0             0             0   \n",
       "466              0             0             0             0             0   \n",
       "121              0             0             0             0             0   \n",
       "1044             0             0             0             0             0   \n",
       "860              0             0             0             0             0   \n",
       "\n",
       "      word_count_5  word_count_6  word_count_7  word_count_8  word_count_9  \\\n",
       "786              0             0             0             0             0   \n",
       "261              0             0             0             0             0   \n",
       "299              0             0             0             0             0   \n",
       "713              0             0             0             0             0   \n",
       "990              0             0             0             0             0   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "330              0             0             0             0             0   \n",
       "466              0             0             0             0             0   \n",
       "121              0             0             0             0             0   \n",
       "1044             0             0             0             0             0   \n",
       "860              0             0             0             0             0   \n",
       "\n",
       "      ...  word_count_6740  word_count_6741  word_count_6742  word_count_6743  \\\n",
       "786   ...                0                0                0                0   \n",
       "261   ...                0                0                0                0   \n",
       "299   ...                0                0                0                0   \n",
       "713   ...                0                0                0                0   \n",
       "990   ...                0                0                0                0   \n",
       "...   ...              ...              ...              ...              ...   \n",
       "330   ...                0                0                0                0   \n",
       "466   ...                0                0                0                0   \n",
       "121   ...                0                0                0                0   \n",
       "1044  ...                0                0                0                0   \n",
       "860   ...                0                0                0                0   \n",
       "\n",
       "      word_count_6744  word_count_6745  word_count_6746  word_count_6747  \\\n",
       "786                 0                0                0                0   \n",
       "261                 0                0                0                0   \n",
       "299                 0                0                0                0   \n",
       "713                 0                0                0                0   \n",
       "990                 0                0                0                0   \n",
       "...               ...              ...              ...              ...   \n",
       "330                 0                0                0                0   \n",
       "466                 0                0                0                0   \n",
       "121                 0                0                0                0   \n",
       "1044                0                0                0                0   \n",
       "860                 0                0                0                0   \n",
       "\n",
       "      word_count_6748  word_count_6749  \n",
       "786                 0                0  \n",
       "261                 0                0  \n",
       "299                 0                0  \n",
       "713                 0                0  \n",
       "990                 0                0  \n",
       "...               ...              ...  \n",
       "330                 0                0  \n",
       "466                 0                0  \n",
       "121                 0                0  \n",
       "1044                0                0  \n",
       "860                 0                0  \n",
       "\n",
       "[842 rows x 6750 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_regression(X_train, y_train, model_name, params, random_state=42):\n",
    "    # create a try and except block to train a logistic regression model\n",
    "    try:\n",
    "        # create a LogisticRegression object\n",
    "        lg_model = LogisticRegression(random_state=random_state, **params)\n",
    "        \n",
    "        # fit the model to the data\n",
    "        lg_model.fit(X_train, np.ravel(y_train))\n",
    "        \n",
    "        # save the model in session state\n",
    "        st.session_state[model_name] = lg_model\n",
    "        \n",
    "        # print confirmation message\n",
    "        print(f\"{model_name} trained and saved in session state.\")\n",
    "        \n",
    "        # return the trained model\n",
    "        return lg_model\n",
    "    \n",
    "    except Exception as e:\n",
    "        # print error message and return None if there was an error\n",
    "        print(f\"Error training logistic regression model: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lg_model trained and saved in session state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(random_state=42)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_logistic_regression(X_train, y_train, 'lg_model', {'max_iter': 100, 'solver': 'lbfgs', 'tol': 1e-4, 'penalty': 'l2'}, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint 6:** In order to fit the training data to the corresponding target, we use the train_sgd_classifer function. This function implements regularized linear models with stochastic gradient descent (SGD) learning, such as SVM, logistic regression, squared hinge and perceptron. We will only be using this function for logistic regression in the assignment, so for the loss parameter, pass “log_loss”. This function takes the following inputs: training features, training targets, a string of the model name, and a dictionary of the hyperparameters to tune during cross validation (X_train, y_train, model_name, params, random_state=42). To prevent overfitting, we will consider ridge regression. Pass “l1” to the penalty parameter. The function returns the trained model (ridge_cv).\n",
    "Perform the following tasks in the train_sgd_classifer function:\n",
    "1.\tCreate a try and except block to train a logistic regression model with Stochastic Gradient Descent algorithm.\n",
    "2.\tCreate a SGDClassifier class object using the random_state and params as input. ``` sgd_model = SGDClassifier(random_state=random_state,\n",
    "loss=params['loss'], penalty=params['penalty'], alpha=params['alpha'], max_iter=params['max_iter'], tol=params['tol'])\n",
    "```\n",
    "3.\tFit the model to the data using the fit() function with input data X_train, y_train.\n",
    "Remember to create a continuous y_train array using np.ravel() function.\n",
    "4.\tSave the model in st.session_state[model_name].\n",
    "5.\tReturn the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sgd_classifier(X_train, y_train, model_name, params, random_state=42):\n",
    "    try:\n",
    "        sgd_model = SGDClassifier(random_state=random_state,\n",
    "                                  loss=params['loss'],\n",
    "                                  penalty=params['penalty'],\n",
    "                                  alpha=params['alpha'],\n",
    "                                  max_iter=params['max_iter'],\n",
    "                                  tol=params['tol'])\n",
    "        sgd_model.fit(X_train, np.ravel(y_train))\n",
    "        st.session_state[model_name] = sgd_model\n",
    "        return sgd_model\n",
    "    except Exception as e:\n",
    "        st.write(\"Error during training:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-22 23:51:13.087 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\ProgramData\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "train_sgd_classifier(X_train, y_train, 'lg_model', {'max_iter': 100, 'solver': 'lbfgs', 'tol': 1e-4, 'penalty': 'l2'}, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint 7:** Now we will train the logistic regression model using cross validation. The train_sgdcv_classifer function takes the following inputs: training features, training targets, a string of the model name, and a dictionary of the hyperparameters to tune during cross validation (X_train, y_train, model_name, params, random_state=42). The function returns the trained model (sgdcv_model). Perform the following tasks in the train_sgdcv_classifer function:\n",
    "Perform the following tasks in the train_sgdcv_classifer function:\n",
    "1.\tCreate a try and except block to train a logistic regression model with Stochastic Gradient Descent algorithm with Repeated K-Fold Cross Validation and search for the optimal parameters with gridsearch.\n",
    "2.\tFind the optimal hyperparameters using GridSearchCV with SGDClassifier as the estimator.\n",
    "``` sgd_cv_model = GridSearchCV(estimator=SGDClassifier(random_state=random_state), param_grid=params, cv=cv_params['n_splits']) ```\n",
    "3.\tFit the model to data using the fit() function\n",
    "4.\tSave\tthe\tcross\tvalidation\tresults\t(sgd_cv_model.cv_results_)\tin st.session_state[‘cv_results_’].\n",
    "5.\tSave the best model estimator in st.session_state[model_name]. ``` st.session_state[model_name] = sgd_cv_model.best_estimator_ ```\n",
    "6.\tReturn the best model estimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sgdcv_classifer(X_train, y_train, model_name, params, cv_params, random_state=42):\n",
    "    try:\n",
    "        # Create SGDClassifier object\n",
    "        sgd_model = SGDClassifier(random_state=random_state, loss=params['loss'], penalty=params['penalty'], \n",
    "                                  alpha=params['alpha'], max_iter=params['max_iter'], tol=params['tol'])\n",
    "        \n",
    "        # Define grid of hyperparameters to search over\n",
    "        param_grid = {'alpha': params['alpha_range'], 'penalty': params['penalty_range']}\n",
    "        \n",
    "        # Define cross validation strategy\n",
    "        cv = RepeatedKFold(n_splits=cv_params['n_splits'], n_repeats=cv_params['n_repeats'], random_state=random_state)\n",
    "        \n",
    "        # Perform GridSearchCV to find optimal hyperparameters\n",
    "        sgd_cv_model = GridSearchCV(estimator=sgd_model, param_grid=param_grid, cv=cv, scoring='accuracy')\n",
    "        sgd_cv_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Save cross validation results and best model estimator\n",
    "        st.session_state['cv_results_'] = sgd_cv_model.cv_results_\n",
    "        st.session_state[model_name] = sgd_cv_model.best_estimator_\n",
    "        # Return best model estimator\n",
    "        return sgd_cv_model.best_estimator_\n",
    "    \n",
    "    except:\n",
    "        st.error('Error: Could not train logistic regression model with SGDClassifier algorithm and cross validation.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDClassifier(alpha=0.01, loss=&#x27;log_loss&#x27;, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier(alpha=0.01, loss=&#x27;log_loss&#x27;, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SGDClassifier(alpha=0.01, loss='log_loss', random_state=42)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'loss': 'log_loss', 'penalty': 'l2', 'alpha': 0.0001, 'max_iter': 1000, 'tol': 1e-3, 'alpha_range': [0.0001, 0.001, 0.01, 0.1], 'penalty_range': ['l2', 'l1', 'elasticnet']}\n",
    "cv_params = {'n_splits': 5, 'n_repeats': 2}\n",
    "\n",
    "train_sgdcv_classifer(X_train, y_train, 'SGDClassifier', params, cv_params, random_state=42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint 8:** Now that we have the trained models, we need to get the coefficients of the trained models and summarize the positive and negative coefficients. Complete the inspect_coefficients function which takes the following inputs: a list of strings of the trained model names, and a list of strings of the model names to print coefficients (trained_models, inspect_models). The inspect_models parameter is given by the user in Streamlit. The function returns a dictionary that contains the coefficients of the selected models, with the keys: 'Logistic\n",
    "Regression',\t'Stochastic\tGradient\tDescent',\tand\t'Stochastic\tGradient\tDescent\twith\tCross\n",
    "Validation'.\n",
    "Perform the following tasks in the inspect_coefficients function:\n",
    "1.\tWrite a for loop through the model names and trained models. ``` for name, model in trained_models.items():```\n",
    "2.\tIn the for loop,\n",
    "a.\tcheck that the model is not None\n",
    "b.\tIf the model is valid, store the coefficients in out_dict[name] using model.coef (same for all models) and display the coefficients.\n",
    "c.\tCompute and print the following values:\n",
    "i.\tTotal number of coefficients\n",
    "ii.\tNumber of positive coefficients\n",
    "iii.\tNumber of negative coefficients\n",
    "3.\tDisplay ‘cv_results_’ in st.session_state[‘cv_results_’] if it exists (from Checkpoint 7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_coefficients(trained_models, inspect_models):\n",
    "    out_dict = {}\n",
    "\n",
    "    for name, model in trained_models.items():\n",
    "        if name in inspect_models and model is not None:\n",
    "            out_dict[name] = model.coef_[0]\n",
    "            print(f\"Model: {name}\")\n",
    "            print(f\"Number of coefficients: {len(model.coef_[0])}\")\n",
    "            print(f\"Number of positive coefficients: {len(model.coef_[0][model.coef_[0] > 0])}\")\n",
    "            print(f\"Number of negative coefficients: {len(model.coef_[0][model.coef_[0] < 0])}\")\n",
    "            print(f\"Coefficients: {model.coef_[0]}\")\n",
    "            print(\"---------------------------------------------------\")\n",
    "    \n",
    "    if 'cv_results_' in st.session_state:\n",
    "        print('cv_results_')\n",
    "        print(st.session_state['cv_results_'])\n",
    "\n",
    "    return out_dict\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4\tTest Regression Models (2 points)\n",
    "\n",
    "The goal of this page is to evaluate the classification models using precision, recall, accuracy, and ROC Curves. First, the user selects the performance metrics for evaluation. Then, they select the classification models to evaluate. Using the aforementioned inputs, two functions, 1) that computes the evaluation metrics using a trained model and 2) displays a ROC Curve using precision and recall. At the end of this page, the user can select a model to deploy on the ‘Deploy App’ page.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint 9:** Next, evaluate metrics for a given logistic regression model. Complete the compute_eval_metrics funcion, which takes the following inputs: the pandas dataframe with training features, the pandas dataframe with true targets, the model to evaluate, and the metrics to evaluate performance (string); 'precision', 'recall', 'accuracy' (X, y_true, model, metrics). The function returns a dictionary which contains the computed metrics of the selected model, with the following structure: {metric1: value1, metric2: value2, ...} (metric_dict).\n",
    "Perform the following tasks in the compute_eval_metrics function:\n",
    "1.\tMake a prediction using the model and input data\n",
    "2.\tWrite a for loop that iterates through metrics, a list containings one or more strings including ‘precision’, ‘recall’, ‘accuracy’\n",
    "a.\tCheck the metric name and compute it based on the string input. For example, if metric=’precision’ then compute the precision on the predicted and input y_true.\n",
    "b.\tStore the result in out_dict[metric_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_eval_metrics(X, y_true, model, metrics):\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    # Initialize dictionary for metric values\n",
    "    metric_dict = {}\n",
    "    \n",
    "    # Compute selected metrics\n",
    "    for metric in metrics:\n",
    "        if metric == 'precision':\n",
    "            metric_dict[metric] = precision_score(y_true, y_pred)\n",
    "        elif metric == 'recall':\n",
    "            metric_dict[metric] = recall_score(y_true, y_pred)\n",
    "        elif metric == 'accuracy':\n",
    "            metric_dict[metric] = accuracy_score(y_true, y_pred)\n",
    "        elif metric == 'roc_auc':\n",
    "            metric_dict[metric] = roc_auc_score(y_true, y_pred)\n",
    "            \n",
    "    return metric_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint 10:** Next, plot the ROC curve between predicted and actual values for model names in trained_models on the training and validation datasets. To do this, complete the plot_roc_curve function which takes the following inputs: training input data, test input data, true targets, predicted targets, trained model names, and the trained models in a dictionary accessed with model name (X_train, X_val, y_train, y_true, trained_models). The function returns the plotted figure, and a dataframe containing the train and validation errors, with the following keys:\n",
    "●\tdf[model_name.__name__ + \" Train Precision\"] = train_precision_all\n",
    "●\tdf[model_name.__name__ + \" Train Recall\"] = train_recall_all\n",
    "●\tdf[model_name.__name__ + \" Validation Precision\"] = val_precision_all\n",
    "●\tdf[model_name.__name__ + \" Validation Recall\"] = val_recall_all\n",
    "Perform the following tasks in the plot_roc_curve function: Write a for loop that iterates through the trained model names with an enumerator (e.g., i) variable to use for plotting\n",
    "1.\tUse the trained model in trained_models[model_name] to:\n",
    "i.\tMake predictions on the train set using predict_proba() function\n",
    "ii.\tMake predictions on the validation set using predict_proba() function\n",
    "iii.\tApply the threshold to the predictions on the training set using the apply_threshold function\n",
    "iv.\tApply the threshold to the predictions on the validation set using the apply_threshold function\n",
    "v.\tCompute precision and recall on the training set using the predictions on the training set (with threshold applied) and the true values (y_train). Use precision_score (set zero_division=1) and recall_score functions. vi.\tCompute precision and recall on validation set using the predictions on the validation set (with threshold applied) and the true values (y_val). Use precision_score (set zero_division=1) and recall_score functions.\n",
    "The apply_threshold function accesses the probabilities of a classification model using the predicted probabilities and a threshold value. The probabilities are a list of arrays where each element has two values (probability that a review is negative and the probability that the review is positive). Note that these probabilities are related in that the probability(positive) = 1 probability(negative); thus, the sum of both probabilities is 1. As a result, we only need to check one probability value and apply the threshold.\n",
    "2.\tPlot a ROC Curves showing the results on training and validation sets using the\n",
    "train_precision_all, train_recall_all, val_precision_all, and val_recall_all. Plot precision on the y-axis and recall on the x-axis (see code snippet below.\n",
    "```\n",
    "fig.add_trace(go.Scatter(x=train_recall_all, y=train_precision_all, name=\"Train\"), row=i+1, col=1) # use enumerated value i to align figures vertically\n",
    "fig.add_trace(go.Scatter(x=val_recall_all, y=val_precision_all, name=\"Validation\"), row=i+1, col=1) # use enumerated value i\n",
    "fig.update_xaxes(title_text=\"Recall\") fig.update_yaxes(title_text='Precision', row=i+1, col=1) # use enumerated value i\n",
    "fig.update_layout(title=model_name+' ROC Curve') ```\n",
    "\n",
    "3. Save the results (train_precision_all, train_recall_all, val_precision_all, and val_recall_all) in df.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "def apply_threshold(y_pred, threshold):\n",
    "    return [1 if pred[1] >= threshold else 0 for pred in y_pred]\n",
    "\n",
    "def plot_roc_curve(X_train, X_val, y_train, y_val, trained_models):\n",
    "    df = pd.DataFrame()\n",
    "    fig = make_subplots(rows=len(trained_models), cols=1)\n",
    "    for i, model_name in enumerate(trained_models.keys()):\n",
    "        model = trained_models[model_name]\n",
    "        y_train_pred = model.predict_proba(X_train)\n",
    "        y_val_pred = model.predict_proba(X_val)\n",
    "        train_pred = apply_threshold(y_train_pred, 0.5)\n",
    "        val_pred = apply_threshold(y_val_pred, 0.5)\n",
    "        train_precision_all = precision_score(y_train, train_pred, zero_division=1)\n",
    "        train_recall_all = recall_score(y_train, train_pred, zero_division=1)\n",
    "        val_precision_all = precision_score(y_val, val_pred, zero_division=1)\n",
    "        val_recall_all = recall_score(y_val, val_pred, zero_division=1)\n",
    "        fig.add_trace(go.Scatter(x=train_recall_all, y=train_precision_all, name=\"Train\"), row=i+1, col=1)\n",
    "        fig.add_trace(go.Scatter(x=val_recall_all, y=val_precision_all, name=\"Validation\"), row=i+1, col=1)\n",
    "        fig.update_xaxes(title_text=\"Recall\")\n",
    "        fig.update_yaxes(title_text='Precision', row=i+1, col=1)\n",
    "        fig.update_layout(title=model_name+' ROC Curve')\n",
    "        df[model_name.__name__ + \" Train Precision\"] = [train_precision_all]\n",
    "        df[model_name.__name__ + \" Train Recall\"] = [train_recall_all]\n",
    "        df[model_name.__name__ + \" Validation Precision\"] = [val_precision_all]\n",
    "        df[model_name.__name__ + \" Validation Recall\"] = [val_recall_all]\n",
    "\n",
    "    fig.show()\n",
    "    return df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5\tDeploy ML Application (1 point)\n",
    "The goal of this page is to deploy a Product Sentiment Classification application that takes a user’s review text as input and predicts whether the review is positive or negative. Your goal is to restore the dataset from the previous page. Then, write the deploy_model() function which uses the selected model from Page C and the input text from the user to predict the review sentiment.\n",
    "Checkpoint 11: Finally, deploy the model! Restore the trained model from st.session_state[‘deploy_model’] and use it to predict the sentiment of the input data. To do this, complete the deploy_model function, which takes the dataframe containing the dataset (df). The function returns the product sentiment, +1 or -1 (Product_sentiment).\n",
    "Perform the following tasks in the deploy_model function:\n",
    "1.\tRestore the model for deployment in st.session_state[‘deploy_model’]\n",
    "2.\tPredict the product sentiment of the input test using the predict function e.g., model.predict(data)\n",
    "The website uses the output of deploy_model to display the product sentiment on the website\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_model(df):\n",
    "    try:\n",
    "        # Restore trained model\n",
    "        model = st.session_state['deploy_model']\n",
    "\n",
    "        # Make predictions on test data\n",
    "        y_pred = model.predict(df)\n",
    "\n",
    "        # Convert predictions to +1 or -1 sentiment\n",
    "        Product_sentiment = [1 if pred == 1 else -1 for pred in y_pred]\n",
    "\n",
    "        return Product_sentiment\n",
    "\n",
    "    except:\n",
    "        st.error('Error: Could not deploy model. Please train the model first.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
